---
title: 'Blog Post 6'
date: 2025-10-12
permalink: /posts/2025/10/blog-post-6/
tags:
  - case study
  - artificial relationships
  - legal action
---

Reacting to a case study about the ramifications of AI companionship.

**Case Study Reading:** [Addictive Intelligence: Understanding Psychological, Legal, and Technical Dimensions of AI Companionship](https://mit-serc.pubpub.org/pub/iopjyxcx/release/2?readingCollection=132bb7af)

Discussion Questions
---
**Part 1: Case Study Summary**  
This case study explores the repercussions of AI companions, which are customized AI chatbots designed to replicate personable human beings. By specifically looking at the Sewell Setzer case, it's shown that this technology lacks regulation and can be dangerous to vulnerable minds.


**Part 2: Going Further**  

1. In the Sewell Setzer case, the AIâ€™s response to suicidal ideation shifted from concern to potentially encouraging harmful behavior. How can companies design AI companions to be emotionally engaging while preventing harmful psychological dependencies? Discuss both technical safeguards (e.g., algorithmic oversight, intervention protocols) and ethical guidelines (e.g., duty of care, transparency).

      In terms of safeguards, I believe that an AI model should be able to detect when a conversation shifts into more sensitive topics to then break character and recommend the user to seek professional or outside help. In other words, I don't think that AI should be used for mental health advice, especially  in terms of specific advice. For ethical guidelines, I think that everytime a program or app is opened, a disclaimer should pop up stating that these models are not real people. Additionally, there should be more safeguards placed in order to ensure that minors do not access to sexual material.

2. How does addiction to AI companions compare with other forms of technology addiction, such as social media or gaming? What unique features make AI companions potentially more addictive? Support your analysis with examples from the case study and other relevant cases.

      AI companions differ from social media and gaming due to its complete separation from society. Most people follow other humans and therefore interact with other humans on social media. For gaming, most people play online with their friends. AI companions do not have even a sliver of this human interaction. So, when an AI addiction develops, users can become more socially isolated and end up feeling more lonely. So they, in turn, use AI more- it can become a visciously isolating cycle.

3. An elderly person finds genuine comfort in an AI companion, alleviating their loneliness, but their family worries this relationship is replacing real human connections. How should we evaluate the benefits versus risks in such cases? What ethical guidelines or intervention strategies might help determine when AI companionship crosses from beneficial to harmful?

      I think its useful knowing why the elderly person sought out AI in the first place. Since they are lonely, that means they are lacking human-seeming interactions and connections. If their family is truly worried about their elderly relative, they might need to step up on the amount of effort they put in to reaching out to said relative. 
      
      I think AI should be seen as a last case scenario, where efforts to socialize with other people are put first. Alternative options should be explored before resulting to AI. The only reason I believe artificial companionship is so popular is because it is extremely accessible. But we need to accept the fact that socialization takes at least a little bit of effort, even if just to get the ball rolling.

4. Current business models incentivize AI companies to maximize user engagement. What alternative economic models could promote healthier AI interactions while maintaining commercial viability? Consider both market-driven solutions (e.g., subscription-based models, ethical AI certifications) and regulatory approaches (e.g., user well-being metrics, engagement caps).

      - Get consent for the data used to train models.
      - Have user based restrictions based on age.
      - Have restrictions on what type of information it can diffuse (i.e. prohibit AI mental health services and suggestions), recognize when a conversation reaches a prohibited state, and provide static, outside resources for dealing with these conversations without AI (especially for AI personalities)
      - Limit the personalization of general chatbots.

5. If you were developing regulations for AI companions, how would you address age restrictions, usage limits, and safety monitoring while respecting user privacy and autonomy? Provide specific examples of how your proposed regulations could have helped prevent incidents like the Sewell Setzer case.

      I believe that each of these platforms with AI companions should have a login system where you must input your birthday. While not a perfect system, the hope is that the content's accessibility will be based on this information. Additonally, I feel that after a certain time limit of usage, a disableable message should pop up prompting the user to, essentially, stop using the platform for the day. With these minor interruptions in combination with the restrictions as described in questions 1 and 4, I feel that these could prevent serious occurences such as the Sewell Setzer case.

**Part 3: Going Beyond**  
With the steady accumulation of legal cases against AI companies for incidents similar to that of Sewell Setzer, do you think AI companies can be left to to self-regulate their models, or is governmental intervention needed? Take, for example, cigarettes. In 2020, the FDA ruled that cigarette packages and advertisements were required to have warnings. Could something similar be necessary for AI in order to prevent more negative and deadly occurences?

**Part 4: Reflection**  
After reading this case study, I feel more informed on why people seek out AI chatbots. As someone who has dealt and is dealing with loneliness, I empathize with these people, but I don't think AI is the way to alleviate that struggle.

I am beginning to think of AI companions as a sort of tragedy. Some people feel so lonely and they want a simple answer to their huge, prevailing problem, so they turn to AI to fix it. It's accessible, it answers almost immediately, it's customizable to be the perfect friend, spouse, or relative. It seems like a great answer, but ultimately it can lead them to increasing isolation. It really is tragic.